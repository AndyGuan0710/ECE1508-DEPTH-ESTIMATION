{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41966,
     "status": "ok",
     "timestamp": 1734097853472,
     "user": {
      "displayName": "Andy Guan",
      "userId": "10636245852350777523"
     },
     "user_tz": -480
    },
    "id": "zZkVsZ_sdZ2b",
    "outputId": "02f02bd5-8628-4ed0-8d29-083997d45341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4456,
     "status": "ok",
     "timestamp": 1734097869551,
     "user": {
      "displayName": "Andy Guan",
      "userId": "10636245852350777523"
     },
     "user_tz": -480
    },
    "id": "pHvJAqws_Yo_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch.nn.modules as nn\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder: Increased capacity with higher channel counts\n",
    "        self.encoder1 = self.conv_block(3, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv4 = self.upconv(1024, 512)\n",
    "        self.decoder4 = self.conv_block(1024, 512)\n",
    "        self.upconv3 = self.upconv(512, 256)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "        self.upconv2 = self.upconv(256, 128)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "        self.upconv1 = self.upconv(128, 64)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        # Final output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Max-pooling for down-sampling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Convolutional block: Two conv layers with BatchNorm and ReLU.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv(self, in_channels, out_channels):\n",
    "        \"\"\"Transposed convolution for up-sampling.\"\"\"\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)  # Output: 64 channels\n",
    "        enc2 = self.encoder2(self.pool(enc1))  # Output: 128 channels\n",
    "        enc3 = self.encoder3(self.pool(enc2))  # Output: 256 channels\n",
    "        enc4 = self.encoder4(self.pool(enc3))  # Output: 512 channels\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool(enc4))  # Output: 1024 channels\n",
    "\n",
    "        # Decoder\n",
    "        dec4 = self.decoder4(torch.cat((self.upconv4(bottleneck), enc4), dim=1))  # Output: 512 channels\n",
    "        dec3 = self.decoder3(torch.cat((self.upconv3(dec4), enc3), dim=1))  # Output: 256 channels\n",
    "        dec2 = self.decoder2(torch.cat((self.upconv2(dec3), enc2), dim=1))  # Output: 128 channels\n",
    "        dec1 = self.decoder1(torch.cat((self.upconv1(dec2), enc1), dim=1))  # Output: 64 channels\n",
    "\n",
    "        # Final depth map\n",
    "        out = self.final(dec1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akLWgFuL_YpF"
   },
   "source": [
    "# Predict Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6228,
     "status": "ok",
     "timestamp": 1734097880135,
     "user": {
      "displayName": "Andy Guan",
      "userId": "10636245852350777523"
     },
     "user_tz": -480
    },
    "id": "dkV1S4jy_YpI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "def temporal_consistency(pred_frame_t, pred_frame_t1):\n",
    "    \"\"\"\n",
    "    Computes temporal consistency (MSE) between consecutive predicted frames.\n",
    "\n",
    "    Args:\n",
    "        pred_frame_t (torch.Tensor): Predicted depth map at time t [1, H, W].\n",
    "        pred_frame_t1 (torch.Tensor): Predicted depth map at time t+1 [1, H, W].\n",
    "\n",
    "    Returns:\n",
    "        float: Temporal consistency loss (MSE).\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.mse_loss(pred_frame_t, pred_frame_t1).item()\n",
    "\n",
    "# Visualization function\n",
    "def visualize_comparison(rgb_frame, pred_depth1, pred_depth2, frame_idx):\n",
    "    \"\"\"\n",
    "    Visualize the comparison between two predicted depth maps.\n",
    "\n",
    "    Args:\n",
    "        rgb_frame (np.array): Original RGB frame.\n",
    "        pred_depth1 (np.array): Predicted depth map from model 1.\n",
    "        pred_depth2 (np.array): Predicted depth map from model 2.\n",
    "        frame_idx (int): Current frame index.\n",
    "    \"\"\"\n",
    "    pred_depth2 /= 10\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Show input frame\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.title(f\"Frame {frame_idx}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Show prediction from model 1\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(pred_depth1, cmap='plasma')\n",
    "    plt.title(\"Model 1 Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Show prediction from model 2\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred_depth2, cmap='plasma')\n",
    "    plt.title(\"Model 2 Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Save or show frame\n",
    "    plt.savefig(f\"comparison_frame_{frame_idx}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734097880135,
     "user": {
      "displayName": "Andy Guan",
      "userId": "10636245852350777523"
     },
     "user_tz": -480
    },
    "id": "KGoSSilrk9j6"
   },
   "outputs": [],
   "source": [
    "# Compare two models on a video\n",
    "def compare_depth_models_no_ground_truth(model1, model2, video_path, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compares two depth estimation models on a video without ground truth.\n",
    "\n",
    "    Args:\n",
    "        model1 (torch.nn.Module): First depth estimation model.\n",
    "        model2 (torch.nn.Module): Second depth estimation model.\n",
    "        video_path (str): Path to the input video.\n",
    "        device (str): Device to run the models on.\n",
    "\n",
    "    Returns:\n",
    "        dict: Temporal consistency metrics for both models.\n",
    "    \"\"\"\n",
    "    # Initialize metrics\n",
    "    temporal_metrics_model1 = []\n",
    "    temporal_metrics_model2 = []\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    prev_depth1, prev_depth2 = None, None\n",
    "    frame_idx = 0\n",
    "    i = 0\n",
    "    # Process each frame\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        print(i)\n",
    "        i+=1\n",
    "\n",
    "        # Preprocess frame (convert to tensor and normalize)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb_frame_tensor = torch.from_numpy(rgb_frame).float().permute(2, 0, 1).unsqueeze(0).to(device) / 255.0\n",
    "\n",
    "        # Predict depth using both models\n",
    "        pred_depth1 = model1(rgb_frame_tensor)\n",
    "        pred_depth2 = model2(rgb_frame_tensor)\n",
    "\n",
    "        # Resize predictions to match input frame\n",
    "        pred_depth1 = torch.nn.functional.interpolate(pred_depth1, size=rgb_frame_tensor.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        pred_depth2 = torch.nn.functional.interpolate(pred_depth2, size=rgb_frame_tensor.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Compute temporal consistency if not the first frame\n",
    "        if prev_depth1 is not None and prev_depth2 is not None:\n",
    "            temporal_metrics_model1.append(temporal_consistency(prev_depth1, pred_depth1))\n",
    "            temporal_metrics_model2.append(temporal_consistency(prev_depth2, pred_depth2))\n",
    "\n",
    "        # Update previous frame predictions\n",
    "        prev_depth1 = pred_depth1.clone()\n",
    "        prev_depth2 = pred_depth2.clone()\n",
    "\n",
    "        # Visualize results (Optional)\n",
    "        visualize_comparison(rgb_frame, pred_depth1[0, 0].detach().cpu().numpy(), pred_depth2[0, 0].detach().cpu().numpy(), frame_idx)\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    # Release video capture\n",
    "    cap.release()\n",
    "\n",
    "    # Compute average temporal consistency\n",
    "    avg_temporal_model1 = np.mean(temporal_metrics_model1)\n",
    "    avg_temporal_model2 = np.mean(temporal_metrics_model2)\n",
    "\n",
    "    return {\"Model1_Temporal\": avg_temporal_model1, \"Model2_Temporal\": avg_temporal_model2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1734097884136,
     "user": {
      "displayName": "Andy Guan",
      "userId": "10636245852350777523"
     },
     "user_tz": -480
    },
    "id": "S9rxD9AYyBmd"
   },
   "outputs": [],
   "source": [
    "class UNet_old(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet_old, self).__init__()\n",
    "        # encoder\n",
    "        self.encoder1 = self.conv_block(3, 32)\n",
    "        self.encoder2 = self.conv_block(32, 64)\n",
    "        self.encoder3 = self.conv_block(64, 128)\n",
    "        self.encoder4 = self.conv_block(128, 256)\n",
    "        # decoder\n",
    "        self.decoder1 = self.conv_block(256, 128)\n",
    "        self.decoder2 = self.conv_block(128, 64)\n",
    "        self.decoder3 = self.conv_block(64, 32)\n",
    "        # one chnnel for depth estimation\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        # up-sample and down-sample\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "\n",
    "        dec1 = self.decoder1(torch.cat((self.up1(enc4), enc3), dim=1))\n",
    "        dec2 = self.decoder2(torch.cat((self.up2(dec1), enc2), dim=1))\n",
    "        dec3 = self.decoder3(torch.cat((self.up3(dec2), enc1), dim=1))\n",
    "        out = self.final(dec3)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10848,
     "status": "ok",
     "timestamp": 1734097896901,
     "user": {
      "displayName": "Andy Guan",
      "userId": "10636245852350777523"
     },
     "user_tz": -480
    },
    "id": "ckkjFgitk9qQ",
    "outputId": "73cc78cc-00b6-4fe5-8255-cfe4c3c8fd6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b102a171f70f>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"/content/gdrive/MyDrive/ECE_1508_project/epoch_149.pth\")\n",
      "<ipython-input-6-b102a171f70f>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_e_30.pth\")\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model1 = UNet_old()\n",
    "checkpoint = torch.load(\"/content/gdrive/MyDrive/ECE_1508_project/epoch_149.pth\")\n",
    "model1.load_state_dict(checkpoint)\n",
    "model1.to(device)\n",
    "model1.eval()\n",
    "\n",
    "model2 = UNet()\n",
    "checkpoint = torch.load(\"/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_e_30.pth\")\n",
    "model2.load_state_dict(checkpoint)\n",
    "model2.to(device)\n",
    "model2.eval()\n",
    "# Path to the video\n",
    "video_path = \"/content/gdrive/MyDrive/ECE_1508_project/test1.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142085,
     "status": "ok",
     "timestamp": 1734098041320,
     "user": {
      "displayName": "Andy Guan",
      "userId": "10636245852350777523"
     },
     "user_tz": -480
    },
    "id": "wGHE4gjgrHZ6",
    "outputId": "370c24f5-a9fc-4825-954e-5237742dd531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "Temporal Consistency Metrics:\n",
      "{'Model1_Temporal': 0.020727838148806334, 'Model2_Temporal': 2.4660542456580873e-05}\n"
     ]
    }
   ],
   "source": [
    "# Compare models\n",
    "metrics = compare_depth_models_no_ground_truth(model1, model2, video_path, device=\"cuda\")\n",
    "print(\"Temporal Consistency Metrics:\")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (multi_model)",
   "language": "python",
   "name": "multi_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
