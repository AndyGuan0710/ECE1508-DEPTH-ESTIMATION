{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZkVsZ_sdZ2b","executionInfo":{"status":"ok","timestamp":1734097853472,"user_tz":-480,"elapsed":41966,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}},"outputId":"02f02bd5-8628-4ed0-8d29-083997d45341"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"pHvJAqws_Yo_","executionInfo":{"status":"ok","timestamp":1734097869551,"user_tz":-480,"elapsed":4456,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","from skimage.metrics import structural_similarity as ssim\n","import torch.nn.modules as nn\n","class UNet(nn.Module):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","\n","        # Encoder: Increased capacity with higher channel counts\n","        self.encoder1 = self.conv_block(3, 64)\n","        self.encoder2 = self.conv_block(64, 128)\n","        self.encoder3 = self.conv_block(128, 256)\n","        self.encoder4 = self.conv_block(256, 512)\n","        self.bottleneck = self.conv_block(512, 1024)\n","\n","        # Decoder\n","        self.upconv4 = self.upconv(1024, 512)\n","        self.decoder4 = self.conv_block(1024, 512)\n","        self.upconv3 = self.upconv(512, 256)\n","        self.decoder3 = self.conv_block(512, 256)\n","        self.upconv2 = self.upconv(256, 128)\n","        self.decoder2 = self.conv_block(256, 128)\n","        self.upconv1 = self.upconv(128, 64)\n","        self.decoder1 = self.conv_block(128, 64)\n","\n","        # Final output layer\n","        self.final = nn.Sequential(\n","            nn.Conv2d(64, 1, kernel_size=1),\n","            nn.Sigmoid()\n","        )\n","\n","        # Max-pooling for down-sampling\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    def conv_block(self, in_channels, out_channels):\n","        \"\"\"Convolutional block: Two conv layers with BatchNorm and ReLU.\"\"\"\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def upconv(self, in_channels, out_channels):\n","        \"\"\"Transposed convolution for up-sampling.\"\"\"\n","        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        # Encoder\n","        enc1 = self.encoder1(x)  # Output: 64 channels\n","        enc2 = self.encoder2(self.pool(enc1))  # Output: 128 channels\n","        enc3 = self.encoder3(self.pool(enc2))  # Output: 256 channels\n","        enc4 = self.encoder4(self.pool(enc3))  # Output: 512 channels\n","\n","        # Bottleneck\n","        bottleneck = self.bottleneck(self.pool(enc4))  # Output: 1024 channels\n","\n","        # Decoder\n","        dec4 = self.decoder4(torch.cat((self.upconv4(bottleneck), enc4), dim=1))  # Output: 512 channels\n","        dec3 = self.decoder3(torch.cat((self.upconv3(dec4), enc3), dim=1))  # Output: 256 channels\n","        dec2 = self.decoder2(torch.cat((self.upconv2(dec3), enc2), dim=1))  # Output: 128 channels\n","        dec1 = self.decoder1(torch.cat((self.upconv1(dec2), enc1), dim=1))  # Output: 64 channels\n","\n","        # Final depth map\n","        out = self.final(dec1)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":567},"id":"aNGj-4zM_YpC","executionInfo":{"status":"error","timestamp":1733731604630,"user_tz":-480,"elapsed":6,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}},"outputId":"01590481-a206-4f9d-f6d4-3bb1528176d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset for temporal consistency training...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] Unable to synchronously open file (unable to open file: name = '/content/gdrive/MyDrive/ECE_1508_project/nyu_depth_v2_labeled.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-84381c66b238>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#/content/gdrive/MyDrive/ECE_1508_project/nyu_depth_v2_labeled.mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading dataset for temporal consistency training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/ECE_1508_project/nyu_depth_v2_labeled.mat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdepths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/content/gdrive/MyDrive/ECE_1508_project/nyu_depth_v2_labeled.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"]}],"source":["import h5py\n","import random\n","# prepare data\n","class TemporalConsistencyDataset(torch.utils.data.Dataset):\n","    def __init__(self, images, depths):\n","        super(TemporalConsistencyDataset, self).__init__()\n","        self.images = images\n","        self.depths = depths\n","\n","    def __len__(self):\n","        return self.images.shape[0]\n","\n","    def __getitem__(self, i):\n","        image = torch.tensor(self.images[i]) / 255.0\n","        depth = torch.tensor(np.expand_dims(self.depths[i], axis=0))\n","        return image, depth\n","\n","\n","#/content/gdrive/MyDrive/ECE_1508_project/nyu_depth_v2_labeled.mat\n","print(\"Loading dataset for temporal consistency training...\")\n","with h5py.File('/content/gdrive/MyDrive/ECE_1508_project/nyu_depth_v2_labeled.mat', 'r') as f:\n","    images = f['images'][:]\n","    depths = f['depths'][:]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"M1azidzq_YpE","executionInfo":{"status":"error","timestamp":1733718449927,"user_tz":-480,"elapsed":434,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}},"outputId":"61ca2c80-63f4-4c5a-aefa-b951e5119b43"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'depths' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-7b7eba4e2dd3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdepths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepths\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTemporalConsistencyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'depths' is not defined"]}],"source":["depths = depths / 10\n","test_num = 200\n","batch_size = 4\n","valid_dataset = TemporalConsistencyDataset(images[:test_num], depths[:test_num])\n","\n","# Create DataLoader\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHbskCO0_YpE"},"outputs":[],"source":["import torch\n","import numpy as np\n","from skimage.metrics import structural_similarity as ssim\n","\n","# Mean Absolute Error (MAE)\n","def mae(pred, target):\n","    return torch.mean(torch.abs(pred - target))\n","\n","# Root Mean Squared Error (RMSE)\n","def rmse(pred, target):\n","    return torch.sqrt(torch.mean((pred - target) ** 2))\n","\n","# Peak Signal-to-Noise Ratio (PSNR)\n","def psnr(pred, target):\n","    mse = torch.mean((pred - target) ** 2)\n","    max_pixel = 1.0  # Since output uses Sigmoid, normalized to [0, 1]\n","    return 10 * torch.log10(max_pixel**2 / mse)\n","\n","# Structural Similarity Index (SSIM)\n","def ssim_metric(pred, target):\n","    \"\"\"\n","    Calculate Structural Similarity Index (SSIM)\n","    \"\"\"\n","    # Convert output from [B, 1, H, W] to [H, W] format\n","    pred_np = pred.cpu().detach().numpy().squeeze()  # [B, 1, H, W] -> [H, W]\n","    target_np = target.cpu().detach().numpy().squeeze()\n","\n","    # Calculate win_size\n","    min_side = min(pred_np.shape)  # Get minimum side length of image\n","    win_size = min(min_side, 7)  # win_size cannot exceed minimum side length, max is 7\n","\n","    # If image size is less than 7x7, set win_size to minimum side length\n","    if min_side < 7:\n","        win_size = min_side if min_side % 2 == 1 else min_side - 1  # Make win_size odd\n","\n","    # Calculate SSIM\n","    return ssim(pred_np, target_np, data_range=target_np.max() - target_np.min(), win_size=win_size)\n","\n","# Evaluate model metrics\n","def evaluate_metrics(model, dataloader, device):\n","    model.eval()  # Set to evaluation mode\n","\n","    mae_total = 0\n","    rmse_total = 0\n","    psnr_total = 0\n","    ssim_total = 0\n","    count = 0  # Used to calculate batch averages\n","\n","    with torch.no_grad():  # No gradient computation needed during evaluation\n","        for (images1, images2), (depth1, depth2) in dataloader:\n","            images1, images2 = images1.to(device), images2.to(device)\n","            depth1, depth2 = depth1.to(device), depth2.to(device)\n","\n","            # Forward pass\n","            outputs1 = model(images1)\n","            outputs2 = model(images2)\n","\n","            # Calculate metrics for each batch\n","            mae_total += mae(outputs1, depth1) + mae(outputs2, depth2)\n","            rmse_total += rmse(outputs1, depth1) + rmse(outputs2, depth2)\n","            psnr_total += psnr(outputs1, depth1) + psnr(outputs2, depth2)\n","\n","            # Calculate SSIM (for each output-target pair)\n","            try:\n","                ssim_total += ssim_metric(outputs1, depth1) + ssim_metric(outputs2, depth2)\n","            except ValueError as e:\n","                # Skip batch if calculation error occurs (e.g. image size too small)\n","                print(f\"Skipping SSIM calculation due to: {e}\")\n","\n","            count += 1\n","\n","    # Calculate averages\n","    mae_avg = mae_total / count\n","    rmse_avg = rmse_total / count\n","    psnr_avg = psnr_total / count\n","    ssim_avg = ssim_total / count if count > 0 else 0  # If there are valid SSIM calculations\n","\n","    return mae_avg, rmse_avg, psnr_avg, ssim_avg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479},"id":"MJhzpVS6_YpF","executionInfo":{"status":"error","timestamp":1733752548993,"user_tz":-480,"elapsed":3210,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}},"outputId":"f70ce5c3-aa53-462a-d1b6-b08430edc9a6"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-330e75e30dc8>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path))  # Load trained model parameters\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-330e75e30dc8>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Validation phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mvalid_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_smooth_1_epoch_21.pth'\u001b[0m  \u001b[0;31m# Replace with your model path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mvalid_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mvalid_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-330e75e30dc8>\u001b[0m in \u001b[0;36mload_trained_model\u001b[0;34m(model_path, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load trained model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move model to specified device (GPU/CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m                 return _load(\n\u001b[0m\u001b[1;32m   1361\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   1813\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m             storage = (\n\u001b[0;32m-> 1772\u001b[0;31m                 \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1773\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_untyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","import tqdm\n","\n","# Load the trained model\n","def load_trained_model(model_path, device):\n","    model = UNet()  # Initialize model\n","    model.load_state_dict(torch.load(model_path))  # Load trained model parameters\n","    model.eval()  # Set to evaluation mode\n","    model.to(device)  # Move model to specified device (GPU/CPU)\n","    return model\n","\n","# Specify device (use GPU if available, otherwise use CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Validation phase\n","valid_model_path = '/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_smooth_1_epoch_21.pth'  # Replace with your model path\n","valid_model = load_trained_model(valid_model_path, device)\n","valid_model.eval()\n","\n","# Initialize metrics\n","mae_avg, rmse_avg, psnr_avg, ssim_avg = 0, 0, 0, 0\n","\n","with torch.no_grad():\n","\n","    # Calculate validation metrics\n","    mae_avg, rmse_avg, psnr_avg, ssim_avg = evaluate_metrics(valid_model, valid_loader, device)\n","\n","print(f\"Validation Metrics - MAE: {mae_avg:.4f}, RMSE: {rmse_avg:.4f}, PSNR: {psnr_avg:.4f}, SSIM: {ssim_avg:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"akLWgFuL_YpF"},"source":["# Predict Video"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479},"id":"aENJZqb-_YpG","executionInfo":{"status":"error","timestamp":1733719403349,"user_tz":-480,"elapsed":5140,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}},"outputId":"3082d501-f13a-4d92-921a-b8d77391ed34"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-03d382149dd3>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path))  # Load trained model parameters\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_smooth_1_epoch_21.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-03d382149dd3>\u001b[0m in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m# Load trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_smooth_1_epoch_21.pth'\u001b[0m  \u001b[0;31m# Replace with your model path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m# Specify video path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-03d382149dd3>\u001b[0m in \u001b[0;36mload_trained_model\u001b[0;34m(model_path, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load trained model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move model to specified device (GPU/CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_smooth_1_epoch_21.pth'"]}],"source":["import torch\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","\n","# Load trained model\n","def load_trained_model(model_path, device):\n","    model = UNet()  # Initialize model\n","    model.load_state_dict(torch.load(model_path))  # Load trained model parameters\n","    model.eval()  # Set to evaluation mode\n","    model.to(device)  # Move model to specified device (GPU/CPU)\n","    return model\n","\n","# Preprocess function: Convert image from BGR to RGB and normalize, ensure consistent size\n","def preprocess_image(image, target_size=(256, 256)):\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image_rgb = cv2.resize(image_rgb, target_size)  # Resize to model's required input size\n","    image_rgb = np.transpose(image_rgb, (2, 0, 1))  # Convert to (C, H, W) format\n","    image_rgb = torch.tensor(image_rgb, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n","    image_rgb /= 255.0  # Normalize to [0, 1]\n","    return image_rgb\n","\n","# Visualize predictions\n","def visualize_predictions(pred_depth1, pred_depth2, frame_idx):\n","    pred_depth1 = pred_depth1.squeeze().cpu().detach().numpy()  # Convert to 2D (H, W)\n","    pred_depth2 = pred_depth2.squeeze().cpu().detach().numpy()  # Convert to 2D (H, W)\n","\n","    # Normalize depth maps to visualization range (0 to 255)\n","    depth_map1 = (pred_depth1 - np.min(pred_depth1)) / (np.max(pred_depth1) - np.min(pred_depth1)) * 255\n","    depth_map2 = (pred_depth2 - np.min(pred_depth2)) / (np.max(pred_depth2) - np.min(pred_depth2)) * 255\n","\n","    depth_map1 = depth_map1.astype(np.uint8)\n","    depth_map2 = depth_map2.astype(np.uint8)\n","\n","    # Visualize as pseudo-color image\n","    depth_map1_colored = cv2.applyColorMap(depth_map1, cv2.COLORMAP_JET)\n","    depth_map2_colored = cv2.applyColorMap(depth_map2, cv2.COLORMAP_JET)\n","\n","    # Display images using matplotlib\n","    if frame_idx % 10 == 0:\n","        print(f\"Visualizing depth for frame {frame_idx}\")\n","\n","        # Display first depth map\n","        plt.figure(figsize=(10, 5))\n","        plt.subplot(1, 2, 1)\n","        plt.title(f\"Predicted Depth - Frame {frame_idx}\")\n","        plt.imshow(depth_map1_colored)\n","        plt.axis('off')\n","\n","        # Display second depth map\n","        plt.subplot(1, 2, 2)\n","        plt.title(f\"Predicted Depth - Frame {frame_idx + 1}\")\n","        plt.imshow(depth_map2_colored)\n","        plt.axis('off')\n","\n","        # Update display\n","        plt.show()\n","\n","def save_depth_video(pred_depths, output_path, frame_size, fps=30):\n","#     fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Try using XVID codec\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use MP4V codec\n","    width, height = frame_size\n","    out_video = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n","\n","    print(f\"Frame size: {frame_size}\")\n","    print(f\"Number of frames: {len(pred_depths)}\")\n","\n","    first_depth = pred_depths[0].cpu().numpy()\n","    print(f\"Depth map shape: {first_depth.shape}\")\n","    print(f\"Depth range: {np.min(first_depth)} to {np.max(first_depth)}\")\n","\n","    for depth in pred_depths:\n","        depth_map = depth.cpu().numpy()\n","        depth_map = (depth_map - np.min(depth_map)) / (np.max(depth_map) - np.min(depth_map)) * 255\n","        depth_map = depth_map.astype(np.uint8)\n","        depth_map = cv2.resize(depth_map, (width, height))\n","        depth_map_colored = cv2.applyColorMap(depth_map, cv2.COLORMAP_JET)\n","\n","        # Write to video\n","        out_video.write(depth_map_colored)\n","\n","    out_video.release()\n","    print(f\"Depth video saved to {output_path}\")\n","\n","# Main function for video depth estimation\n","def test_video_depth_estimation(video_path, model, device, output_video_path=None):\n","    # Open video file\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(f\"Error: Could not open video {video_path}\")\n","        return\n","\n","    # Get video width, height and fps\n","    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","    # Store predicted depth maps\n","    pred_depths = []\n","\n","    # Process video frame by frame\n","    prev_frame_rgb = None\n","    frame_idx = 0\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break  # Video finished\n","\n","        # Preprocess image\n","        current_frame_rgb = preprocess_image(frame)\n","        current_frame_rgb = current_frame_rgb.to(device)\n","\n","        if prev_frame_rgb is not None:\n","            # Estimate depth for both frames\n","            with torch.no_grad():\n","                pred_depth1 = model(prev_frame_rgb)  # Depth map for first frame\n","                pred_depth2 = model(current_frame_rgb)  # Depth map for second frame\n","\n","            # Visualize predicted depth maps\n","            visualize_predictions(pred_depth1, pred_depth2, frame_idx)\n","\n","            # Save predicted depth maps\n","            pred_depths.append(pred_depth1.squeeze().cpu())  # Store first frame's depth map\n","            pred_depths.append(pred_depth2.squeeze().cpu())  # Store second frame's depth map\n","\n","            frame_idx += 1\n","\n","        # Update previous frame\n","        prev_frame_rgb = current_frame_rgb\n","\n","    cap.release()\n","\n","    # Save output video if path is specified\n","    if output_video_path is not None:\n","        save_depth_video(pred_depths, output_video_path, (frame_width, frame_height), fps)\n","\n","# Specify device (use GPU if available, otherwise use CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load trained model\n","model_path = '/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_smooth_1_epoch_21.pth'  # Replace with your model path\n","model = load_trained_model(model_path, device)\n","\n","# Specify video path\n","video_path = '/content/gdrive/MyDrive/ECE_1508_project/test1.mp4'  #\n","\n","# Specify output video path\n","output_video_path = '/home/ub/Documents/Al/model_final/save/output_depth_video1.mp4'  # Can be set to None if you don't want to save video\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dkV1S4jy_YpI","executionInfo":{"status":"ok","timestamp":1734097880135,"user_tz":-480,"elapsed":6228,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}}},"outputs":[],"source":["import torch\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","def temporal_consistency(pred_frame_t, pred_frame_t1):\n","    \"\"\"\n","    Computes temporal consistency (MSE) between consecutive predicted frames.\n","\n","    Args:\n","        pred_frame_t (torch.Tensor): Predicted depth map at time t [1, H, W].\n","        pred_frame_t1 (torch.Tensor): Predicted depth map at time t+1 [1, H, W].\n","\n","    Returns:\n","        float: Temporal consistency loss (MSE).\n","    \"\"\"\n","    return torch.nn.functional.mse_loss(pred_frame_t, pred_frame_t1).item()\n","\n","# Visualization function\n","def visualize_comparison(rgb_frame, pred_depth1, pred_depth2, frame_idx):\n","    \"\"\"\n","    Visualize the comparison between two predicted depth maps.\n","\n","    Args:\n","        rgb_frame (np.array): Original RGB frame.\n","        pred_depth1 (np.array): Predicted depth map from model 1.\n","        pred_depth2 (np.array): Predicted depth map from model 2.\n","        frame_idx (int): Current frame index.\n","    \"\"\"\n","    pred_depth2 /= 10\n","    plt.figure(figsize=(12, 6))\n","\n","    # Show input frame\n","    plt.subplot(1, 3, 1)\n","    plt.imshow(rgb_frame)\n","    plt.title(f\"Frame {frame_idx}\")\n","    plt.axis(\"off\")\n","\n","    # Show prediction from model 1\n","    plt.subplot(1, 3, 2)\n","    plt.imshow(pred_depth1, cmap='plasma')\n","    plt.title(\"Model 1 Prediction\")\n","    plt.axis(\"off\")\n","\n","    # Show prediction from model 2\n","    plt.subplot(1, 3, 3)\n","    plt.imshow(pred_depth2, cmap='plasma')\n","    plt.title(\"Model 2 Prediction\")\n","    plt.axis(\"off\")\n","\n","    # Save or show frame\n","    plt.savefig(f\"comparison_frame_{frame_idx}.png\")\n","    plt.close()"]},{"cell_type":"code","source":["# Compare two models on a video\n","def compare_depth_models_no_ground_truth(model1, model2, video_path, device=\"cuda\"):\n","    \"\"\"\n","    Compares two depth estimation models on a video without ground truth.\n","\n","    Args:\n","        model1 (torch.nn.Module): First depth estimation model.\n","        model2 (torch.nn.Module): Second depth estimation model.\n","        video_path (str): Path to the input video.\n","        device (str): Device to run the models on.\n","\n","    Returns:\n","        dict: Temporal consistency metrics for both models.\n","    \"\"\"\n","    # Initialize metrics\n","    temporal_metrics_model1 = []\n","    temporal_metrics_model2 = []\n","\n","    # Open video file\n","    cap = cv2.VideoCapture(video_path)\n","    prev_depth1, prev_depth2 = None, None\n","    frame_idx = 0\n","    i = 0\n","    # Process each frame\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        print(i)\n","        i+=1\n","\n","        # Preprocess frame (convert to tensor and normalize)\n","        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        rgb_frame_tensor = torch.from_numpy(rgb_frame).float().permute(2, 0, 1).unsqueeze(0).to(device) / 255.0\n","\n","        # Predict depth using both models\n","        pred_depth1 = model1(rgb_frame_tensor)\n","        pred_depth2 = model2(rgb_frame_tensor)\n","\n","        # Resize predictions to match input frame\n","        pred_depth1 = torch.nn.functional.interpolate(pred_depth1, size=rgb_frame_tensor.shape[-2:], mode='bilinear', align_corners=False)\n","        pred_depth2 = torch.nn.functional.interpolate(pred_depth2, size=rgb_frame_tensor.shape[-2:], mode='bilinear', align_corners=False)\n","\n","        # Compute temporal consistency if not the first frame\n","        if prev_depth1 is not None and prev_depth2 is not None:\n","            temporal_metrics_model1.append(temporal_consistency(prev_depth1, pred_depth1))\n","            temporal_metrics_model2.append(temporal_consistency(prev_depth2, pred_depth2))\n","\n","        # Update previous frame predictions\n","        prev_depth1 = pred_depth1.clone()\n","        prev_depth2 = pred_depth2.clone()\n","\n","        # Visualize results (Optional)\n","        visualize_comparison(rgb_frame, pred_depth1[0, 0].detach().cpu().numpy(), pred_depth2[0, 0].detach().cpu().numpy(), frame_idx)\n","\n","        frame_idx += 1\n","\n","    # Release video capture\n","    cap.release()\n","\n","    # Compute average temporal consistency\n","    avg_temporal_model1 = np.mean(temporal_metrics_model1)\n","    avg_temporal_model2 = np.mean(temporal_metrics_model2)\n","\n","    return {\"Model1_Temporal\": avg_temporal_model1, \"Model2_Temporal\": avg_temporal_model2}"],"metadata":{"id":"KGoSSilrk9j6","executionInfo":{"status":"ok","timestamp":1734097880135,"user_tz":-480,"elapsed":2,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class UNet_old(nn.Module):\n","    def __init__(self):\n","        super(UNet_old, self).__init__()\n","        # encoder\n","        self.encoder1 = self.conv_block(3, 32)\n","        self.encoder2 = self.conv_block(32, 64)\n","        self.encoder3 = self.conv_block(64, 128)\n","        self.encoder4 = self.conv_block(128, 256)\n","        # decoder\n","        self.decoder1 = self.conv_block(256, 128)\n","        self.decoder2 = self.conv_block(128, 64)\n","        self.decoder3 = self.conv_block(64, 32)\n","        # one chnnel for depth estimation\n","        self.final = nn.Sequential(\n","            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n","            nn.Conv2d(16, 1, kernel_size=3, padding=1),\n","        )\n","\n","        # up-sample and down-sample\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.up3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n","\n","    def conv_block(self, in_channels, out_channels):\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        enc1 = self.encoder1(x)\n","        enc2 = self.encoder2(self.pool(enc1))\n","        enc3 = self.encoder3(self.pool(enc2))\n","\n","        enc4 = self.encoder4(self.pool(enc3))\n","\n","        dec1 = self.decoder1(torch.cat((self.up1(enc4), enc3), dim=1))\n","        dec2 = self.decoder2(torch.cat((self.up2(dec1), enc2), dim=1))\n","        dec3 = self.decoder3(torch.cat((self.up3(dec2), enc1), dim=1))\n","        out = self.final(dec3)\n","\n","        return out"],"metadata":{"id":"S9rxD9AYyBmd","executionInfo":{"status":"ok","timestamp":1734097884136,"user_tz":-480,"elapsed":604,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Load models\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model1 = UNet_old()\n","checkpoint = torch.load(\"/content/gdrive/MyDrive/ECE_1508_project/epoch_149.pth\")\n","model1.load_state_dict(checkpoint)\n","model1.to(device)\n","model1.eval()\n","\n","model2 = UNet()\n","checkpoint = torch.load(\"/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_e_30.pth\")\n","model2.load_state_dict(checkpoint)\n","model2.to(device)\n","model2.eval()\n","# Path to the video\n","video_path = \"/content/gdrive/MyDrive/ECE_1508_project/test1.mp4\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ckkjFgitk9qQ","executionInfo":{"status":"ok","timestamp":1734097896901,"user_tz":-480,"elapsed":10848,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}},"outputId":"73cc78cc-00b6-4fe5-8255-cfe4c3c8fd6d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-b102a171f70f>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(\"/content/gdrive/MyDrive/ECE_1508_project/epoch_149.pth\")\n","<ipython-input-6-b102a171f70f>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(\"/content/gdrive/MyDrive/ECE_1508_project/supervised_edge_aware_temporal_e_30.pth\")\n"]}]},{"cell_type":"code","source":["# Compare models\n","metrics = compare_depth_models_no_ground_truth(model1, model2, video_path, device=\"cuda\")\n","print(\"Temporal Consistency Metrics:\")\n","print(metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGHE4gjgrHZ6","executionInfo":{"status":"ok","timestamp":1734098041320,"user_tz":-480,"elapsed":142085,"user":{"displayName":"Andy Guan","userId":"10636245852350777523"}},"outputId":"370c24f5-a9fc-4825-954e-5237742dd531"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","Temporal Consistency Metrics:\n","{'Model1_Temporal': 0.020727838148806334, 'Model2_Temporal': 2.4660542456580873e-05}\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python (multi_model)","language":"python","name":"multi_model"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}